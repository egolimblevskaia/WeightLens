# Transcoder Configuration for our Llama 3.2 1B ReLU transcoders
model_name: "meta-llama/Llama-3.2-1B"
feature_input_hook: "hook_resid_mid"
feature_output_hook: 'mlp.hook_out'

transcoders:
  - id: "Llama-3.2-131k-relu-0"
    layer: 0
    filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_0.safetensors?revision=new-training"

  - id: "Llama-3.2-131k-relu-1"
    layer: 1
    filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_1.safetensors?revision=new-training"

  - id: "Llama-3.2-131k-relu-2"
    layer: 2
    filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_2.safetensors?revision=new-training"

  - id: "Llama-3.2-131k-relu-3"
    layer: 3
    filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_3.safetensors?revision=new-training"

  - id: "Llama-3.2-131k-relu-4"
    layer: 4
    filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_4.safetensors?revision=new-training"

  # - id: "Llama-3.2-131k-relu-5"
  #   layer: 5
  #   filepath: "hf://mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/layer_5.safetensors?revision=new-training"